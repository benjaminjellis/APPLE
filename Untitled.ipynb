{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backtest is a class used to backtest and thus evaluate saved models on new data.\n",
    "Models are tested on a randomly selected 70% of 19/20 season data, the class then updates\n",
    "the model log\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from core.scaler import scale_df_with_params\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\"\"\"\n",
    "Backtest is a class used to backtest and thus evaluate saved models on new data.\n",
    "Models are tested on a randomly selected 70% of 19/20 season data, the class then updates\n",
    "the model log\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def log_update(model_id, results, model_log):\n",
    "    \"\"\"\n",
    "    Function used to update each model entry in the model log once the\n",
    "    model has been backtested against new data\n",
    "    :param model_id: str\n",
    "            - uniqe model id to update log for\n",
    "    :param results: tuple\n",
    "            - tuple of length 2 with the test loss and test accuracy in that order\n",
    "    :param model_log: dataframe\n",
    "            - the model log as a dataframe to update\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    # get the row number for the model\n",
    "    row = model_log[model_log['Model ID'] == model_id].index\n",
    "    # update test loos\n",
    "    model_log.at[row, \"Test Loss\"] = results[0]\n",
    "    model_log.at[row, \"Test Acc\"] = results[1]\n",
    "\n",
    "\n",
    "def load_and_aggregate(dir_to_aggregate):\n",
    "\n",
    "    \"\"\"\n",
    "    Used to load all data files from specified dir and aggregate them into one dataframe\n",
    "    :param dir_to_aggregate: str\n",
    "            path of the directory to load and aggregate files form\n",
    "    :return: dataframe\n",
    "            dataframe of the loaded and aggregated data files\n",
    "    \"\"\"\n",
    "\n",
    "    def read_json_or_csv(filepath):\n",
    "        path_ignore, extension = filepath.split(\".\")\n",
    "        if extension == \"json\":\n",
    "            read_df = pd.read_json(filepath)\n",
    "            return read_df\n",
    "        elif extension == \"csv\":\n",
    "            read_df = pd.read_csv(filepath)\n",
    "            return read_df\n",
    "        else:\n",
    "            raise Exception(\"Mined data file type not recognised: \" + str(filepath))\n",
    "\n",
    "    # list files in dir_to_aggregate\n",
    "    files_in_dir = [f for f in listdir(dir_to_aggregate) if isfile(join(dir_to_aggregate, f))]\n",
    "\n",
    "    # remove .DS_Store from the list of files in dir_to_aggregate\n",
    "    while '.DS_Store' in files_in_dir:\n",
    "        files_in_dir.remove('.DS_Store')\n",
    "\n",
    "    # load first file in the dir_to_aggregate\n",
    "    aggregated_dataset = read_json_or_csv(dir_to_aggregate + files_in_dir[0])\n",
    "\n",
    "    # remove first file from the list of mined data files\n",
    "    files_in_dir.pop(0)\n",
    "\n",
    "    # loop through remaining data files and append them to self.mined_data\n",
    "    for file in files_in_dir:\n",
    "        loaded_file = read_json_or_csv(dir_to_aggregate + file)\n",
    "        aggregated_dataset = pd.concat([aggregated_dataset, loaded_file], ignore_index = True)\n",
    "\n",
    "    return aggregated_dataset\n",
    "\n",
    "\n",
    "class Backtest(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor loads the model log, aggregates data to make backtesting dataset, maps locations of saved models\n",
    "        \"\"\"\n",
    "        # load model log\n",
    "        self.path = str(Path().absolute())\n",
    "        self.model_log_loc = self.path + \"/saved_models/\"\n",
    "        self.model_log_path = self.model_log_loc + \"model_log.csv\"\n",
    "        self.model_log = pd.read_csv(self.model_log_path)\n",
    "        # get location of saved models\n",
    "        self.saved_models_dir = self.path + \"/saved_models/\"\n",
    "\n",
    "        # load and aggreate all mined data\n",
    "        mined_data_dir = self.path + \"/data/mined_data/\"\n",
    "        mined_data_aggregated = load_and_aggregate(mined_data_dir)\n",
    "\n",
    "        # load and aggreate all ftrs\n",
    "        ftrs_dir = self.path + \"/data/results/\"\n",
    "        ftrs_aggregated = load_and_aggregate(ftrs_dir)\n",
    "\n",
    "        # inner join to get raw backtesing data\n",
    "        self.raw_backtesting_data = pd.merge(mined_data_aggregated, ftrs_aggregated, on = [\"HomeTeam\", \"AwayTeam\"])\n",
    "\n",
    "    def model(self, model_id):\n",
    "        \"\"\"\n",
    "        Method to backtest a single model\n",
    "        :param model_id:  str\n",
    "            - uniqe model id to backtest\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        print(\"Backtesting model \" + str(model_id))\n",
    "        # grab entry for requested model\n",
    "        model_details = self.model_log[self.model_log[\"Model ID\"] == model_id]\n",
    "        # get model type for requested model\n",
    "        model_type = int(model_details[\"Model type\"].iloc[0])\n",
    "\n",
    "        # load scaled\n",
    "        sclaer_coeffs_loc = self.saved_models_dir + model_id + \"/\" + model_id + \"_coeffs.csv\"\n",
    "        # grab the model\n",
    "        loaded_model = tf.keras.models.load_model(self.saved_models_dir + model_id + \"/\" + model_id + \".h5\")\n",
    "        exp_features_df = pd.read_csv(self.saved_models_dir + model_id + \"/\" + model_id + \".csv\")\n",
    "        exp_features = exp_features_df[\"columns\"].to_list()\n",
    "\n",
    "        # add FTR to the list of expected features\n",
    "        exp_features.append(\"FTR\")\n",
    "        rawdata = self.raw_backtesting_data\n",
    "\n",
    "        # code below is replicated from predict.py, turn this into a def and use in both classes\n",
    "        if model_type == 2 or model_type == 3:\n",
    "            ats = pd.get_dummies(rawdata[\"AwayTeam\"], prefix = \"at\")\n",
    "            hts = pd.get_dummies(rawdata[\"HomeTeam\"], prefix = \"ht\")\n",
    "            rawdata = pd.concat([rawdata, ats], axis = 1, sort = False)\n",
    "            rawdata = pd.concat([rawdata, hts], axis = 1, sort = False)\n",
    "\n",
    "            rawdata.pop(\"AwayTeam\")\n",
    "            rawdata.pop(\"HomeTeam\")\n",
    "        # this block is also used in predict.py, this should be packed up into a function and stored\n",
    "        # somewhere in core\n",
    "        try:\n",
    "            backtesting_data = rawdata[exp_features]\n",
    "        except KeyError:\n",
    "            key_present = list(rawdata.columns)\n",
    "            keys_missing = []\n",
    "\n",
    "            for key in exp_features:\n",
    "                if key not in key_present:\n",
    "                    keys_missing.append(key)\n",
    "\n",
    "            length = len(keys_missing)\n",
    "            track = 0\n",
    "            shape = rawdata.shape\n",
    "\n",
    "            for keys in keys_missing:\n",
    "                if keys[:2] == \"at\" or \"ht\":\n",
    "                    track += 1\n",
    "\n",
    "            print(colored(\"Attempting to deal with missing datapoint(s)....\", 'red'))\n",
    "\n",
    "            entry = np.zeros((shape[0], 1))\n",
    "\n",
    "            if length == track:\n",
    "                print(colored(\"Fixing input data....\", 'red'))\n",
    "                for key in keys_missing:\n",
    "                    rawdata[key] = entry\n",
    "                try:\n",
    "                    backtesting_data = rawdata[exp_features]\n",
    "                    print(colored(\"Input data fixed\", 'green'))\n",
    "                except KeyError:\n",
    "                    print(colored(\"ERROR - Missing datapoint(s): \", 'red'))\n",
    "                    print(colored(\"Attempt was made to fix input data, but it wasn't possible\", 'red'))\n",
    "\n",
    "            else:\n",
    "                print(colored(\"ERROR - input data cannot be handled\", 'red'))\n",
    "                print(\n",
    "                    colored(model_id + \" can't be used to make predicitons wihtout the above, datapoint(s)\",\n",
    "                            \"red\"))\n",
    "                raise Exception\n",
    "\n",
    "        # categorical encoding for FTR col\n",
    "        result_cleanup = {\"FTR\": {\"H\": 0, \"A\": 1, \"D\": 2}}\n",
    "        backtesting_data.replace(result_cleanup, inplace = True)\n",
    "\n",
    "        # remove labels\n",
    "        backtesting_data_labels = backtesting_data.pop(\"FTR\")\n",
    "\n",
    "        # scale the features\n",
    "        backtesting_data = scale_df_with_params(backtesting_data, sclaer_coeffs_loc)\n",
    "\n",
    "        # turn labels and features in numpy arrays for passing to model\n",
    "        backtesting_data_labels = backtesting_data_labels.to_numpy()\n",
    "        backtesting_data = backtesting_data.to_numpy()\n",
    "        # use model to evaluate\n",
    "        results = loaded_model.evaluate(backtesting_data, backtesting_data_labels, batch_size = 50)\n",
    "        # now update the log with the results\n",
    "        print(results)\n",
    "        #log_update(model_id = model_id, results = results, model_log = self.model_log)\n",
    "\n",
    "    def models(self, models):\n",
    "        \"\"\"\n",
    "        Method to backtest one of more models\n",
    "        :param models: list of str\n",
    "                - a list of models to backtest\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        for model in models:\n",
    "            self.model(model_id = model)\n",
    "\n",
    "    def all(self):\n",
    "        \"\"\"\n",
    "        Method to backtest all models\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        # get all the mode ids as a list\n",
    "        models = self.model_log[\"Model ID\"].to_list()\n",
    "        # perform back testing on all models\n",
    "        for model in models:\n",
    "            self.model(model_id = model)\n",
    "\n",
    "    def commit_log_updates(self):\n",
    "        \"\"\"\n",
    "        Method to commit log updates made by any instance to file\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        self.model_log.to_csv(self.model_log_path, index_label = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting model df6bbd02-befe-11ea-a872-acde48001122\n",
      "\u001b[31mAttempting to deal with missing datapoint(s)....\u001b[0m\n",
      "\u001b[31mFixing input data....\u001b[0m\n",
      "\u001b[32mInput data fixed\u001b[0m\n",
      "40/40 [==============================] - 0s 2ms/sample - loss: 1.0204 - accuracy: 0.5000\n",
      "[1.020442247390747, 0.5]\n"
     ]
    }
   ],
   "source": [
    "new = Backtest()\n",
    "new.model(model_id=\"df6bbd02-befe-11ea-a872-acde48001122\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APPLE",
   "language": "python",
   "name": "apple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
